---
layout: post
title: "Intelligence artificielle vs démocratie"
date: 2017-01-23 18:08
comments: true
categories: ["Lecture"]
image: "2017-01-23-math-destruction.jpg" 
---

Dans les derniers jours, Martin Lessard a publié [deux](http://ici.radio-canada.ca/nouvelle/1004752/triplex-probleme-de-lintelligence-artificielle) [articles](http://ici.radio-canada.ca/nouvelle/1007099/triplex-intelligence-artificielle-hier-menace-humanite-aujourdhui-majordome)<sup>1</sup> auxquels je peux difficilement ne pas réagir considérant mes positions sur le sujet et ma dernière lecture en date: [Weapons of math destruction](https://weaponsofmathdestructionbook.com/).

Ça fera peut-être l’objet d’un billet plus articulé mais voici quelques idées lancées à la va-vite.

La majorité des écrits actuels sur le sujet vient d’experts. Bien qu’ils puissent avoir un regard critique sur l’utilisation de l’intelligence artificielle et des modèles d’analyse, ils demeurent pris dans un biais insurmontable.

La conclusion, symbolisée par le titre d'un des deux articles, est qu'ultimement c’est l’humain, notamment parce qu’il veut comprendre comment ça marche, la limite à l’utilisation d’une technologie somme toute salvatrice. Pourtant, comme le démontre très bien Cathy O’Neil -elle-même avec son biais d’expert, accepter ces modèles sans se questionner outre mesure est la recette pour un désastre annoncé. On ne parle pas de Terminator ou de Blade Runner. On parle de systèmes fonctionnant dans l’ombre de notre société et tirant les ficèles selon des paramètres incompréhensibles pour la grande majorité des gens.

Les modèles développés autour de l’intelligence artificielle (et des technologies moins avancées d’analyse de données) visent à maximiser certains paramètres dans un système complexe: la vie. 

Premièrement le simple choix des paramètres à maximiser peut vite tourner au cauchemar. Certes, cela peut sembler bénin quand on parle d’apprendre à une voiture à se garer ou à gérer la température d’une maison (exemple utilisés dans les articles). Mais quand il s’agit d’accès à l’éducation, au logement, à des traitements médicaux, c’est autrement plus problématique. La mise en oeuvre de tels modèles se fait généralement par les entités qui sont déjà en situation de force et qui ont ainsi tout loisir d’optimiser ce qui les arrange, sans que quiconque puisse vraiment argumenter.

Ensuite la mise en oeuvre elle-même d’un système artificiel est pavée de danger: les données sont par nature des “proxies”, représentation simplifiée de la réalité qui peuvent la pervertir, les cycles de rétroaction du système lui permettant d’apprendre de ses erreurs sont parfois inexistants ou très partiels, la validité mathématique est souvent compromise (pour être efficace, bien des modèles ont besoin de millions de *data points*), etc. Cathy O’Neil  propose 4 critères pour définir un modèle pouvant devenir un “Weapon of math destruction”. Je vous laisse lire le livre pour découvrir lesdits critères.

Dans des systèmes simples ou arbitraires, il peut être aisé de trouver les paramètres à optimiser et appliquer un modèle adéquate. Paradoxalement, les véhicules autonomes entrent dans la catégorie des cas assez simples: le premier paramètre à optimiser est la sécurité. L’absence d’accident est un paramètre facile à évaluer (et là encore, avec le [dilemme du tramway](https://fr.wikipedia.org/wiki/Dilemme_du_tramway), certains soulèvent des enjeux difficiles à “optimiser”) et tout le monde peut voir le résultat.

Mais tel qu’expliqué, la majorité des systèmes d’intelligence artificielle agissent dans l’ombre dans des situations complexes, affectant nos vies, nous affichant quotidiennement une réalité différente de notre voisin (arrêtons de nous fixer sur Facebook, c’est maintenant généralisé), voué à ségréguer la population d’une manière bien plus efficace et perverse que l’Apartheid.

> “Data is not going away. Nor are computers --much less mathematics. Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. (...) Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral.”
> <div class="attrib">Weapons of math destruction. How big data increases inequality and threatens democracy. Cathy O’Neil.</div>

Dire que le problème de l’intelligence artificielle c’est l’humain ou que ce n’est pas une vraiment menace, ne fait pas justice à l’ensemble des problématiques en jeu. Pas de scénario apocalyptique, de prise de contrôle des machines, mais quelque chose de beaucoup plus pernicieux où ceux en mesure de comprendre ce qu’est l’intelligence artificielle seront aussi ceux qui en bénéficieront, au détriment des autres. Cathy O’Neil propose quelques pistes, insuffisantes, mais qui au moins changent de l’approche candide entourant actuellement ce domaine. 

---

<sup>1</sup> Évidemment, rien de personnel. Les billets de Martin, par la clarté des arguments, le fait qu’ils sont passé sous mon radar et le souhait que je voulais commenter le livre que je venais de terminer, tombaient juste à point.